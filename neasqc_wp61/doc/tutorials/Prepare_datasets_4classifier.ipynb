{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5810cdde",
   "metadata": {},
   "source": [
    "# Data preparation for classifier training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb331959",
   "metadata": {},
   "source": [
    "This notebook shows how to prepare the data for the classifier trainig.\n",
    "\n",
    "    Input: \n",
    "        Text file with two TAB separated columns. The first column contains the label, the second - the sentence.\n",
    "    Output: \n",
    "        .json files (with train and test split) with embeddings obtained from the different pre-trained embedding models:\n",
    "            1) word-level fastText embeddings: model cc.en.300.bin\n",
    "                (https://fasttext.cc/docs/en/crawl-vectors.html)\n",
    "            2) sentence-level transformer embeddings: model all-mpnet-base-v2\n",
    "                (https://www.sbert.net/docs/pretrained_models.html#model-overview)\n",
    "            3) sentence-level transformer embeddings: model all-distilroberta-v1\n",
    "                (https://www.sbert.net/docs/pretrained_models.html#model-overview)\n",
    "            4) sentence-level BERT cased embeddings: model BERT-Base, Cased\n",
    "                (https://github.com/google-research/bert#pre-trained-models)\n",
    "            5) sentence-level BERT uncased embeddings: model BERT-Base, Uncased\n",
    "                (https://github.com/google-research/bert#pre-trained-models)\n",
    "                \n",
    "            \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f8b90c",
   "metadata": {},
   "source": [
    "## 1. Filtering text examples by length and syntactical structure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cda8c669",
   "metadata": {},
   "source": [
    "*** At first we tokenize text examples. Then we parse text ignoring lines containing more than 6 tokens (thouse lines are excluded from the further processing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de2f2a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import json\n",
    "import csv\n",
    "import re\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a21e7881",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lambeq import SpacyTokeniser\n",
    "from lambeq import BobcatParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c7d2e328",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokeniser = SpacyTokeniser()\n",
    "bobcat_parser = BobcatParser() #BobcatParser(device=0) # if GPU is not vailable then BobcatParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cab4024b",
   "metadata": {},
   "outputs": [],
   "source": [
    "datadir = \"../../data/datasets\"\n",
    "dsName='reviews'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16cb2c91",
   "metadata": {},
   "source": [
    "Change the variables according to your dataset!\n",
    "Specify if your dataset has field values in the first row and what are names of the classification field and the text field. Also specify field delimiter symbol."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "86f94603",
   "metadata": {},
   "outputs": [],
   "source": [
    "fieldnamesinfile=True\n",
    "classfield=\"Score\"\n",
    "txtfield=\"Summary\"\n",
    "firstsentence=False #Try to process only the first sentence for the texts longer than 6 tokens\n",
    "delimiter=','"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "399b6f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = f\"{datadir}/{dsName}.csv\"\n",
    "output_file = f\"{datadir}/{dsName}_alltrees.tsv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d74c74eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(input_file, encoding=\"utf8\", newline='') as csvfile, open(output_file, \"w\", encoding=\"utf8\") as tsvfile:\n",
    "        if fieldnamesinfile != False: \n",
    "            news_reader = csv.DictReader(csvfile, delimiter=delimiter, quotechar='\"')\n",
    "        else:\n",
    "            news_reader = csv.DictReader(csvfile, delimiter=delimiter, fieldnames = [classfield, txtfield], quotechar='\"')        \n",
    "        processed_summaries = set()\n",
    "        norm_process_params = [\"perl\", \"../../data/data_processing/scripts/normalize-punctuation.perl\",\"-b\",\"-l\", \"en\"]\n",
    "        norm_process = subprocess.Popen(norm_process_params, stdin=subprocess.PIPE, stdout=subprocess.PIPE, close_fds=True)\n",
    "        for row in news_reader:\n",
    "            score = row[classfield]\n",
    "            if score == '0':\n",
    "                continue\n",
    "            summary = row[txtfield].replace(\"\\n\",\" \").replace(\"\\t\",\" \").replace(\"\\r\",\" \")\n",
    "            summary = re.sub('@[^\\s]+ ','',summary)\n",
    "            norm_process.stdin.write(summary.encode('utf-8'))\n",
    "            norm_process.stdin.write('\\n'.encode('utf-8'))\n",
    "            norm_process.stdin.flush()\n",
    "            norm_summary = norm_process.stdout.readline().decode(\"utf-8\").rstrip()\n",
    "            tok_summary = \" \".join(tokeniser.tokenise_sentences([norm_summary])[0])\n",
    "\n",
    "            if len(tok_summary.split())>6:\n",
    "                if firstsentence==True:\n",
    "                    tok_summary = re.sub('^([^\\.!?]+).+','\\\\1',tok_summary)\n",
    "                    if len(tok_summary.split())>6:\n",
    "                        continue\n",
    "                else:\n",
    "                    continue\n",
    "            if tok_summary in processed_summaries:\n",
    "                continue\n",
    "            sent_type = ''\n",
    "            processed_summaries.add(tok_summary)\n",
    "            try:\n",
    "                sent_type = 's'\n",
    "                result = bobcat_parser.sentence2tree(tok_summary).to_json()\n",
    "                str1=re.sub('\\'(rule|text)\\':\\s[\\\"\\'][^\\s]+[\\\"\\']','',str(result))\n",
    "                str1=re.sub('\\'(type|children)\\':\\s+','',str1)\n",
    "                str1=re.sub('[\\{\\},\\']','',str1)\n",
    "                str1=re.sub('\\s+([\\[\\]])','\\\\1',str1)\n",
    "                sent_type = str1\n",
    "            except:\n",
    "                sent_type = ''\n",
    "            print(\"{0}\\t{1}\\t{2}\".format(score, tok_summary,sent_type),file=tsvfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c91550fd",
   "metadata": {},
   "source": [
    "Output is 3-column filtered file containing class, text and syntactical structure of the text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "327b37c3",
   "metadata": {},
   "source": [
    "*** Next we chose text examples according to the set of prespecified syntactical structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e45ad3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = f\"{datadir}/{dsName}_alltrees.tsv\"\n",
    "output_file = f\"{datadir}/{dsName}_filtered.tsv\"\n",
    "tags_file = f\"{datadir}/validtrees.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac28ddfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "filterlist = open(tags_file).read().splitlines()\n",
    "print(filterlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d04105",
   "metadata": {},
   "outputs": [],
   "source": [
    "  \n",
    "with open(input_file, \"r\", encoding=\"utf8\") as ifile, open(output_file, \"w\", encoding=\"utf8\") as ofile:\n",
    "    tsv_reader = csv.DictReader(ifile, fieldnames=['Class','Txt','Tag'], delimiter=\"\\t\", quotechar='\"')\n",
    "    for item in tsv_reader:\n",
    "        if item['Tag'] in filterlist:\n",
    "            print(\"{0}\\t{1}\\t{2}\".format(item['Class'],item['Txt'],item['Tag']),file=ofile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c08e7cf6",
   "metadata": {},
   "source": [
    "Output is 3-column by syntax filtered file containing class, text and syntactical structure of the text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26cd3236",
   "metadata": {},
   "source": [
    "## 2. Splitting examples in train and test sets and acquiring embedding vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f38e86",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from typing import List, Tuple, Dict\n",
    "sys.path.append(\"../../data/data_processing/data_vectorisation/\")\n",
    "from Embeddings import Embeddings\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "906ce8fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3347c837",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpack_data(data: List[Tuple[str, str]]) -> List[Dict[str, str]]:\n",
    "    return [{\n",
    "        \"sentence\": sentence,\n",
    "        \"class\": sentence_type,\n",
    "    } for sentence, sentence_type in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a62bae00",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = f\"{datadir}/{dsName}_filtered.tsv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e644074",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset: List[Tuple[str, str]] = []\n",
    "datasettag={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c31a0d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        cols=line.split('\\t')\n",
    "        if len(cols) == 3:\n",
    "            sent = cols[1].rstrip()\n",
    "            dataset.append((sent, cols[0].rstrip()))\n",
    "            datasettag[sent] = cols[2].rstrip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a6548f",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = [item[1] for item in dataset]\n",
    "classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55556c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, tmp_data = train_test_split(dataset, train_size=0.8, random_state=1, stratify=classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc7a7025",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = [item[1] for item in tmp_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52cf448d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf688f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data, dev_data = train_test_split(tmp_data, train_size=0.5, random_state=1, stratify=classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8623128e",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_result = defaultdict(list)\n",
    "for element in test_data:\n",
    "    my_result[element[1]].append(element[0])\n",
    "\n",
    "my_result = dict(my_result)\n",
    "result_dictionary = dict()\n",
    "\n",
    "for key in my_result:\n",
    "    result_dictionary[key] = len(list(set(my_result[key]))) / len(test_data)\n",
    "print(f\"*** Proportion of classes in {len(test_data)} examples of test data ***\")\n",
    "print(json.dumps(result_dictionary, indent=4, sort_keys=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb795927",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_result = defaultdict(list)\n",
    "for element in train_data:\n",
    "    my_result[element[1]].append(element[0])\n",
    "\n",
    "my_result = dict(my_result)\n",
    "result_dictionary = dict()\n",
    "\n",
    "for key in my_result:\n",
    "    result_dictionary[key] = len(list(set(my_result[key]))) / len(train_data)\n",
    "print(f\"*** Proportion of classes in {len(train_data)} examples of train data ***\")\n",
    "print(json.dumps(result_dictionary, indent=4, sort_keys=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeea33fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = unpack_data(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a608dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = unpack_data(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e61761",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_data = unpack_data(dev_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8287e074",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"{datadir}/{dsName}_filtered_train.tsv\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for item in train_data:\n",
    "        item[\"tag\"] = datasettag[item[\"sentence\"]]\n",
    "        f.write(f'{item[\"class\"]}\\t{item[\"sentence\"]}\\t{item[\"tag\"]}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d0cf97",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"{datadir}/{dsName}_filtered_test.tsv\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for item in test_data:\n",
    "        item[\"tag\"] = datasettag[item[\"sentence\"]]\n",
    "        f.write(f'{item[\"class\"]}\\t{item[\"sentence\"]}\\t{item[\"tag\"]}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f8713ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"{datadir}/{dsName}_filtered_dev.tsv\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for item in dev_data:\n",
    "        item[\"tag\"] = datasettag[item[\"sentence\"]]\n",
    "        f.write(f'{item[\"class\"]}\\t{item[\"sentence\"]}\\t{item[\"tag\"]}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae1403c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ObtainEmbeddings(train_data, test_data, dev_data, key, path, embtype):\n",
    "    vectorizer = Embeddings(path=path,embtype=embtype)\n",
    "        \n",
    "    cnt = 0\n",
    "    print(f\"\\n*** Getting vectors for {len(train_data)} examples of train data ***\", end='\\n')\n",
    "    for item in train_data:\n",
    "        item[\"sentence_vectorized\"] = vectorizer.getEmbeddingVector(item[\"sentence\"])\n",
    "        cnt = cnt + 1\n",
    "        if cnt % 50 == 0:\n",
    "            print (str(cnt),end=' ')\n",
    "                \n",
    "    cnt = 0\n",
    "    print(f\"\\n*** Getting vectors for {len(test_data)} examples of test data ***\", end='\\n')\n",
    "    for item in test_data:\n",
    "        item[\"sentence_vectorized\"] = vectorizer.getEmbeddingVector(item[\"sentence\"])\n",
    "        cnt = cnt + 1\n",
    "        if cnt % 50 == 0:\n",
    "            print (str(cnt),end=' ')\n",
    "            \n",
    "    cnt = 0\n",
    "    print(f\"\\n*** Getting vectors for {len(dev_data)} examples of development data ***\", end='\\n')\n",
    "    for item in dev_data:\n",
    "        item[\"sentence_vectorized\"] = vectorizer.getEmbeddingVector(item[\"sentence\"])\n",
    "        cnt = cnt + 1\n",
    "        if cnt % 50 == 0:\n",
    "            print (str(cnt),end=' ')\n",
    "        \n",
    "    with open(f\"{datadir}/{dsName}_{key}.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump({\"train_data\": train_data, \"test_data\": test_data, \"dev_data\": dev_data}, f, indent=2, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f79df79",
   "metadata": {},
   "outputs": [],
   "source": [
    "ObtainEmbeddings(train_data, test_data, dev_data, 'FASTTEXT', 'cc.en.300.bin', 'fasttext')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c56978",
   "metadata": {},
   "outputs": [],
   "source": [
    "ObtainEmbeddings(train_data, test_data, dev_data, 'all-mpnet-base', 'all-mpnet-base-v2', 'transformer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d91eaefa",
   "metadata": {},
   "outputs": [],
   "source": [
    "ObtainEmbeddings(train_data, test_data, dev_data, 'all-distilroberta', 'all-distilroberta-v1', 'transformer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e8e0d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ObtainEmbeddings(train_data, test_data, dev_data, 'BERT_UNCASED', 'bert-base-uncased', 'bert')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e3a6c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "ObtainEmbeddings(train_data, test_data, dev_data, 'BERT_CASED', 'bert-base-cased', 'bert')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b23aec8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
