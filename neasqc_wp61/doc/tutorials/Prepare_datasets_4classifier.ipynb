{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5810cdde",
   "metadata": {},
   "source": [
    "# Data preparation for classifier training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb331959",
   "metadata": {},
   "source": [
    "This notebook shows how to prepare the data for the classifier trainig.\n",
    "\n",
    "    Input: \n",
    "        Text file with two TAB separated columns. The first column contains the label, the second - the sentence.\n",
    "    Output: \n",
    "        .json files (with train and test split) with embeddings obtained from the different pre-trained embedding models:\n",
    "            1) word-level fastText embeddings: model cc.en.300.bin\n",
    "                (https://fasttext.cc/docs/en/crawl-vectors.html)\n",
    "            2) sentence-level transformer embeddings: model all-mpnet-base-v2\n",
    "                (https://www.sbert.net/docs/pretrained_models.html#model-overview)\n",
    "            3) sentence-level transformer embeddings: model all-distilroberta-v1\n",
    "                (https://www.sbert.net/docs/pretrained_models.html#model-overview)\n",
    "            4) sentence-level BERT cased embeddings: model BERT-Base, Cased\n",
    "                (https://github.com/google-research/bert#pre-trained-models)\n",
    "            5) sentence-level BERT uncased embeddings: model BERT-Base, Uncased\n",
    "                (https://github.com/google-research/bert#pre-trained-models)\n",
    "                \n",
    "        (.._traintest.json files contain train/test split without embedding vectors)\n",
    "            \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f8b90c",
   "metadata": {},
   "source": [
    "## 1. Filtering text examples by length and syntactical structure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cda8c669",
   "metadata": {},
   "source": [
    "*** At first we tokenize text examples. Then we parse text ignoring lines containing more than 6 tokens (thouse lines are excluded from the further processing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de2f2a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import json\n",
    "import csv\n",
    "import re\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a21e7881",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lambeq import SpacyTokeniser\n",
    "from lambeq import BobcatParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c7d2e328",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokeniser = SpacyTokeniser()\n",
    "bobcat_parser = BobcatParser() #BobcatParser(device=0) # if GPU is not vailable then BobcatParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cab4024b",
   "metadata": {},
   "outputs": [],
   "source": [
    "datadir = \"../../data/datasets\"\n",
    "dsName='RAW_interactions'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16cb2c91",
   "metadata": {},
   "source": [
    "Change the variables according to your dataset!\n",
    "Specify if your dataset has field values in the first row and what are names of the classification field and the text field. Also specify field delimiter symbol."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "86f94603",
   "metadata": {},
   "outputs": [],
   "source": [
    "fieldnamesinfile=True\n",
    "classfield=\"rating\"\n",
    "txtfield=\"review\"\n",
    "firstsentence=False #Try to process only the first sentence for the texts longer than 6 tokens\n",
    "delimiter=','"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "399b6f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = f\"{datadir}/{dsName}.csv\"\n",
    "output_file = f\"{datadir}/withtags_{dsName}.tsv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d74c74eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(input_file, encoding=\"utf8\", newline='') as csvfile, open(output_file, \"w\", encoding=\"utf8\") as tsvfile:\n",
    "        if fieldnamesinfile != False: \n",
    "            news_reader = csv.DictReader(csvfile, delimiter=delimiter, quotechar='\"')\n",
    "        else:\n",
    "            news_reader = csv.DictReader(csvfile, delimiter=delimiter, fieldnames = [classfield, txtfield], quotechar='\"')        \n",
    "        processed_summaries = set()\n",
    "        norm_process_params = [\"perl\", \"../../data/data_processing/scripts/normalize-punctuation.perl\",\"-b\",\"-l\", \"en\"]\n",
    "        norm_process = subprocess.Popen(norm_process_params, stdin=subprocess.PIPE, stdout=subprocess.PIPE, close_fds=True)\n",
    "        for row in news_reader:\n",
    "            score = row[classfield]\n",
    "            if score == '0':\n",
    "                continue\n",
    "            summary = row[txtfield].replace(\"\\n\",\" \").replace(\"\\t\",\" \").replace(\"\\r\",\" \")\n",
    "            summary = re.sub('@[^\\s]+ ','',summary)\n",
    "            norm_process.stdin.write(summary.encode('utf-8'))\n",
    "            norm_process.stdin.write('\\n'.encode('utf-8'))\n",
    "            norm_process.stdin.flush()\n",
    "            norm_summary = norm_process.stdout.readline().decode(\"utf-8\").rstrip()\n",
    "            tok_summary = \" \".join(tokeniser.tokenise_sentences([norm_summary])[0])\n",
    "\n",
    "            if len(tok_summary.split())>6:\n",
    "                if firstsentence==True:\n",
    "                    tok_summary = re.sub('^([^\\.!?]+).+','\\\\1',tok_summary)\n",
    "                    if len(tok_summary.split())>6:\n",
    "                        continue\n",
    "                else:\n",
    "                    continue\n",
    "            if tok_summary in processed_summaries:\n",
    "                continue\n",
    "            sent_type = ''\n",
    "            processed_summaries.add(tok_summary)\n",
    "            try:\n",
    "                sent_type = 's'\n",
    "                result = bobcat_parser.sentence2tree(tok_summary).to_json()\n",
    "                str1=re.sub('\\'(rule|text)\\':\\s[\\\"\\'][^\\s]+[\\\"\\']','',str(result))\n",
    "                str1=re.sub('\\'(type|children)\\':\\s+','',str1)\n",
    "                str1=re.sub('[\\{\\},\\']','',str1)\n",
    "                str1=re.sub('\\s+([\\[\\]])','\\\\1',str1)\n",
    "                sent_type = str1\n",
    "            except:\n",
    "                sent_type = ''\n",
    "            print(\"{0}\\t{1}\\t{2}\".format(score, tok_summary,sent_type),file=tsvfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c91550fd",
   "metadata": {},
   "source": [
    "Output is 3-column filtered file containing class, text and syntactical structure of the text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "327b37c3",
   "metadata": {},
   "source": [
    "*** Next we chose text examples according to the set of prespecified syntactical structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5e45ad3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = f\"{datadir}/withtags_{dsName}.tsv\"\n",
    "output_file = f\"{datadir}/{dsName}.tsv\"\n",
    "tags_file = f\"{datadir}/tags.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ac28ddfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['n[(n/n)   n[(n/n)   n]]', 'n[(n/n)   n[(n/n)   n[(n/n)   n]]]', 'n[n[n[(n/n)   n]] (n\\\\\\\\n)[((n\\\\\\\\n)/n)   n[n[(n/n)   n]]]]', 'n[(n/n)[((n/n)/(n/n))   (n/n)] n]', 'n[n[n[(n/n)   n]] (n\\\\\\\\n)[((n\\\\\\\\n)/n)   n[(n/n)   n]]]']\n"
     ]
    }
   ],
   "source": [
    "filterlist = open(tags_file).read().splitlines()\n",
    "print(filterlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "94d04105",
   "metadata": {},
   "outputs": [],
   "source": [
    "  \n",
    "with open(input_file, \"r\", encoding=\"utf8\") as ifile, open(output_file, \"w\", encoding=\"utf8\") as ofile:\n",
    "    tsv_reader = csv.DictReader(ifile, fieldnames=['Class','Txt','Tag'], delimiter=\"\\t\", quotechar='\"')\n",
    "    for item in tsv_reader:\n",
    "        if item['Tag'] in filterlist:\n",
    "            print(\"{0}\\t{1}\\t{2}\".format(item['Class'],item['Txt'],item['Tag']),file=ofile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c08e7cf6",
   "metadata": {},
   "source": [
    "Output is 3-column by syntax filtered file containing class, text and syntactical structure of the text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26cd3236",
   "metadata": {},
   "source": [
    "## 2. Splitting examples in train and test sets and acquiring embedding vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "45f38e86",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from typing import List, Tuple, Dict\n",
    "sys.path.append(\"../../data/data_processing/data_vectorisation/\")\n",
    "from Embeddings import Embeddings\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "906ce8fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3347c837",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpack_data(data: List[Tuple[str, str]]) -> List[Dict[str, str]]:\n",
    "    return [{\n",
    "        \"sentence\": sentence,\n",
    "        \"class\": sentence_type,\n",
    "    } for sentence, sentence_type in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a62bae00",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = f\"{datadir}/{dsName}.tsv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3e644074",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset: List[Tuple[str, str]] = []\n",
    "datasettag={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c31a0d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        cols=line.split('\\t')\n",
    "        if len(cols) == 3:\n",
    "            sent = cols[1].rstrip()\n",
    "            dataset.append((sent, cols[0].rstrip()))\n",
    "            datasettag[sent] = cols[2].rstrip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "76a6548f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['5',\n",
       " '5',\n",
       " '5',\n",
       " '5',\n",
       " '3',\n",
       " '5',\n",
       " '5',\n",
       " '5',\n",
       " '5',\n",
       " '5',\n",
       " '5',\n",
       " '4',\n",
       " '5',\n",
       " '5',\n",
       " '4',\n",
       " '5',\n",
       " '5',\n",
       " '5',\n",
       " '5',\n",
       " '5',\n",
       " '5',\n",
       " '5',\n",
       " '5',\n",
       " '4',\n",
       " '5',\n",
       " '5',\n",
       " '5',\n",
       " '5',\n",
       " '5',\n",
       " '5',\n",
       " '5',\n",
       " '5',\n",
       " '5',\n",
       " '5',\n",
       " '5',\n",
       " '5',\n",
       " '5',\n",
       " '3',\n",
       " '5',\n",
       " '5',\n",
       " '5',\n",
       " '5',\n",
       " '3',\n",
       " '5',\n",
       " '5',\n",
       " '5',\n",
       " '4',\n",
       " '4',\n",
       " '5',\n",
       " '5',\n",
       " '4',\n",
       " '4',\n",
       " '5',\n",
       " '5',\n",
       " '5',\n",
       " '4',\n",
       " '5',\n",
       " '4',\n",
       " '5',\n",
       " '5',\n",
       " '5',\n",
       " '5',\n",
       " '5',\n",
       " '3',\n",
       " '5',\n",
       " '5',\n",
       " '5',\n",
       " '5',\n",
       " '5',\n",
       " '5',\n",
       " '5',\n",
       " '5',\n",
       " '4',\n",
       " '5',\n",
       " '4',\n",
       " '5',\n",
       " '5',\n",
       " '5',\n",
       " '1',\n",
       " '5',\n",
       " '5',\n",
       " '4',\n",
       " '5',\n",
       " '5',\n",
       " '5',\n",
       " '5',\n",
       " '5',\n",
       " '5',\n",
       " '5',\n",
       " '4',\n",
       " '5',\n",
       " '5',\n",
       " '5',\n",
       " '4',\n",
       " '2',\n",
       " '5',\n",
       " '5',\n",
       " '5',\n",
       " '3',\n",
       " '5',\n",
       " '5',\n",
       " '5',\n",
       " '5',\n",
       " '5',\n",
       " '4',\n",
       " '5',\n",
       " '5',\n",
       " '5',\n",
       " '1',\n",
       " '1',\n",
       " '4',\n",
       " '4',\n",
       " '1',\n",
       " '4',\n",
       " '4',\n",
       " '4',\n",
       " '5',\n",
       " '5',\n",
       " '3',\n",
       " '4',\n",
       " '5',\n",
       " '5',\n",
       " '5',\n",
       " '5',\n",
       " '5',\n",
       " '5',\n",
       " '5',\n",
       " '5',\n",
       " '5',\n",
       " '5',\n",
       " '5',\n",
       " '5',\n",
       " '5',\n",
       " '5',\n",
       " '5',\n",
       " '5',\n",
       " '5',\n",
       " '5',\n",
       " '5',\n",
       " '5',\n",
       " '5',\n",
       " '5',\n",
       " '5',\n",
       " '5',\n",
       " '5',\n",
       " '4',\n",
       " '5',\n",
       " '3',\n",
       " '5',\n",
       " '5',\n",
       " '5',\n",
       " '5',\n",
       " '5',\n",
       " '4',\n",
       " '4',\n",
       " '5',\n",
       " '5',\n",
       " '5',\n",
       " '5',\n",
       " '2',\n",
       " '5',\n",
       " '3',\n",
       " '5',\n",
       " '5',\n",
       " '5',\n",
       " '5',\n",
       " '5',\n",
       " '3',\n",
       " '5',\n",
       " '5',\n",
       " '5',\n",
       " '3',\n",
       " '5',\n",
       " '5',\n",
       " '5',\n",
       " '5',\n",
       " '5',\n",
       " '1',\n",
       " '5',\n",
       " '5',\n",
       " '5',\n",
       " '4',\n",
       " '5',\n",
       " '4',\n",
       " '5',\n",
       " '4',\n",
       " '3',\n",
       " '5',\n",
       " '5',\n",
       " '5',\n",
       " '4',\n",
       " '5',\n",
       " '2',\n",
       " '1',\n",
       " '4',\n",
       " '5',\n",
       " '5']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes = [item[1] for item in dataset]\n",
    "classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "55556c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = train_test_split(dataset, train_size=0.9, random_state=1, stratify=classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8623128e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Proportion of classes in 20 examples of test data ***\n",
      "{\n",
      "    \"1\": 0.05,\n",
      "    \"3\": 0.05,\n",
      "    \"4\": 0.15,\n",
      "    \"5\": 0.75\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "my_result = defaultdict(list)\n",
    "for element in test_data:\n",
    "    my_result[element[1]].append(element[0])\n",
    "\n",
    "my_result = dict(my_result)\n",
    "result_dictionary = dict()\n",
    "\n",
    "for key in my_result:\n",
    "    result_dictionary[key] = len(list(set(my_result[key]))) / len(test_data)\n",
    "print(f\"*** Proportion of classes in {len(test_data)} examples of test data ***\")\n",
    "print(json.dumps(result_dictionary, indent=4, sort_keys=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cb795927",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Proportion of classes in 177 examples of train data ***\n",
      "{\n",
      "    \"1\": 0.02824858757062147,\n",
      "    \"2\": 0.01694915254237288,\n",
      "    \"3\": 0.05649717514124294,\n",
      "    \"4\": 0.14689265536723164,\n",
      "    \"5\": 0.751412429378531\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "my_result = defaultdict(list)\n",
    "for element in train_data:\n",
    "    my_result[element[1]].append(element[0])\n",
    "\n",
    "my_result = dict(my_result)\n",
    "result_dictionary = dict()\n",
    "\n",
    "for key in my_result:\n",
    "    result_dictionary[key] = len(list(set(my_result[key]))) / len(train_data)\n",
    "print(f\"*** Proportion of classes in {len(train_data)} examples of train data ***\")\n",
    "print(json.dumps(result_dictionary, indent=4, sort_keys=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "aeea33fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = unpack_data(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "51a608dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = unpack_data(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8287e074",
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in train_data:\n",
    "    item[\"tag\"] = datasettag[item[\"sentence\"]]\n",
    "for item in train_data:\n",
    "    item[\"tag\"] = datasettag[item[\"sentence\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "100c3a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"{datadir}/{dsName}_traintest.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump({\"train_data\": train_data, \"test_data\": test_data}, f, indent=1, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ae1403c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ObtainEmbeddings(train_data, test_data, key, path, embtype):\n",
    "    vectorizer = Embeddings(path=path,embtype=embtype)\n",
    "        \n",
    "    cnt = 0\n",
    "    print(f\"\\n*** Getting vectors for {len(train_data)} examples of train data ***\", end='\\n')\n",
    "    for item in train_data:\n",
    "        item[\"sentence_vectorized\"] = vectorizer.getEmbeddingVector(item[\"sentence\"])\n",
    "        cnt = cnt + 1\n",
    "        if cnt % 50 == 0:\n",
    "            print (str(cnt),end=' ')\n",
    "                \n",
    "    cnt = 0\n",
    "    print(f\"\\n*** Getting vectors for {len(test_data)} examples of test data ***\", end='\\n')\n",
    "    for item in test_data:\n",
    "        item[\"sentence_vectorized\"] = vectorizer.getEmbeddingVector(item[\"sentence\"])\n",
    "        cnt = cnt + 1\n",
    "        if cnt % 50 == 0:\n",
    "            print (str(cnt),end=' ')\n",
    "        \n",
    "    with open(f\"{datadir}/{dsName}_{key}.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump({\"train_data\": train_data, \"test_data\": test_data}, f, indent=2, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2f79df79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cc.en.300.bin loaded!\n",
      "\n",
      "*** Getting vectors for 177 examples of train data ***\n",
      "50 100 150 \n",
      "*** Getting vectors for 20 examples of test data ***\n"
     ]
    }
   ],
   "source": [
    "ObtainEmbeddings(train_data, test_data, 'FASTTEXT', 'cc.en.300.bin', 'fasttext')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "97c56978",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all-mpnet-base-v2 loaded!\n",
      "\n",
      "*** Getting vectors for 177 examples of train data ***\n",
      "50 100 150 \n",
      "*** Getting vectors for 20 examples of test data ***\n"
     ]
    }
   ],
   "source": [
    "ObtainEmbeddings(train_data, test_data, 'all-mpnet-base', 'all-mpnet-base-v2', 'transformer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d91eaefa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all-distilroberta-v1 loaded!\n",
      "\n",
      "*** Getting vectors for 177 examples of train data ***\n",
      "50 100 150 \n",
      "*** Getting vectors for 20 examples of test data ***\n"
     ]
    }
   ],
   "source": [
    "ObtainEmbeddings(train_data, test_data, 'all-distilroberta', 'all-distilroberta-v1', 'transformer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6e8e0d4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert-base-uncased loaded!\n",
      "\n",
      "*** Getting vectors for 177 examples of train data ***\n",
      "50 100 150 \n",
      "*** Getting vectors for 20 examples of test data ***\n"
     ]
    }
   ],
   "source": [
    "ObtainEmbeddings(train_data, test_data, 'BERT_UNCASED', 'bert-base-uncased', 'bert')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8e3a6c78",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert-base-cased loaded!\n",
      "\n",
      "*** Getting vectors for 177 examples of train data ***\n",
      "50 100 150 \n",
      "*** Getting vectors for 20 examples of test data ***\n"
     ]
    }
   ],
   "source": [
    "ObtainEmbeddings(train_data, test_data, 'BERT_CASED', 'bert-base-cased', 'bert')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b23aec8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
