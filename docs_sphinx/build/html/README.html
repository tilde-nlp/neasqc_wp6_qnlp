
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>Quantum Natural Language Processing : NEASQC WP6.1 &#8212; WP6_QNLP 0.3 documentation</title>
    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="_static/alabaster.css" />
    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="prev" title="Welcome to WP6_QNLP’s documentation!" href="index.html" />
   
  <link rel="stylesheet" href="_static/custom.css" type="text/css" />
  
  
  <meta name="viewport" content="width=device-width, initial-scale=0.9, maximum-scale=0.9" />

  </head><body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          

          <div class="body" role="main">
            
  <section id="quantum-natural-language-processing-neasqc-wp6-1">
<h1>Quantum Natural Language Processing : NEASQC WP6.1<a class="headerlink" href="#quantum-natural-language-processing-neasqc-wp6-1" title="Permalink to this headline">¶</a></h1>
<section id="installing-locally">
<h2>Installing locally<a class="headerlink" href="#installing-locally" title="Permalink to this headline">¶</a></h2>
<section id="obtaining-a-local-copy-of-the-code-repository">
<h3>Obtaining a local copy of the code repository<a class="headerlink" href="#obtaining-a-local-copy-of-the-code-repository" title="Permalink to this headline">¶</a></h3>
<p>In order to run the code locally, you will need to obtain a copy of the repository. To this end, you can either fork the repository or clone it.</p>
<section id="cloning">
<h4>Cloning<a class="headerlink" href="#cloning" title="Permalink to this headline">¶</a></h4>
<p>We here detail the procedure to be followed for cloning.</p>
<ol>
  <li>Open the code repository in your browser.</li>
  <li>Open the drop-down menu on the leftClick on the 'Switch branches/tags' button to select v0.3 branch.</li>
  <li>Click on the green code button and choose the cloning method you want to use, GitHub provides detailes steps for each method (HTTPS, SSH, etc).</li>
  <li>Open a terminal on your computer and navigate to the directory you wish to clone the repository into. </li>
  <li>Run the following command in your terminal:
      <pre><code>$ git clone &ltcopied_url&gt</pre></code></li>
  <li>Navigate into the cloned repository by using 
     <pre><code>$ cd WP6_QNLP</pre></code> </li>
  <li>Run the following command in your terminal: 
      <pre><code>$ git checkout v0.3</pre></code></li>
</ol>
</section>
</section>
<section id="creating-a-new-environment-and-installing-required-packages">
<h3>Creating a new environment and installing required packages<a class="headerlink" href="#creating-a-new-environment-and-installing-required-packages" title="Permalink to this headline">¶</a></h3>
<section id="python-version">
<h4>Python version<a class="headerlink" href="#python-version" title="Permalink to this headline">¶</a></h4>
<p>The Python version required to run the scripts and notebooks of this repository is Python 3.10. Due to the presence of myQLM , only <a class="reference external" href="https://www.python.org/downloads/macos/">python.org</a> and brew python distributions are supported and <code class="docutils literal notranslate"><span class="pre">pyenv</span></code> won’t work.</p>
<ol>
<li>If Python3.10 hasn't been installed (<em><strong>using brew</strong></em>) yet, or Python3.10 has been installed using any other method:
  <ol>
    <li>We run the following command on the terminal to install it on your local device.
      <pre><code>$ brew install python@3.10</pre></code></li>
    <li>By running the following command on the terminal, we make sure that we will link the recently installed Python3.10 to the environmental variable <em><strong>python3.10</em></strong>.
      <pre><code>$ brew link --overwrite python@3.10</pre></code>
    We may get an error if there was any other environmental variable named <em><strong>python3.10</em></strong>. In that case we must remove the variable from the PATH with the command: 
      <pre><code>$ unset python3.10</pre></code>
    and then use brew link command again.</li>
  </ol>
</li>
<li>If Python3.10 has been already installed (<em><strong>using brew</em></strong>):
  <ol>
    <li>We make sure that we have it linked to the the environmental variable <em><strong>python3.10</em></strong> using the command shown on section 1.2. A warning message will appear if we have it already linked (we can ignore it).</li>
    <li>We make sure that there are no packages installed on the global Python by running the command: 
      <pre><code>$ python3.10 -m pip list</pre></code>
    In the case where there were packages installed on the global Python we should uninstall them with the command: 
      <pre><code>$ python3.10 -m pip uninstall &ltundesired package&gt</pre></code></li>
  </ol>
</li>
</ol>
</section>
<section id="virtual-environment">
<h4>Virtual environment<a class="headerlink" href="#virtual-environment" title="Permalink to this headline">¶</a></h4>
<p>Once you have cloned the repository, we recommend creating a virtual environment to run the code and notebooks. There are several tools to create a virtual environment, here we describe one of them and encourage users to resort to whichever approach is most familiar.</p>
<ul>
  <li>To create a virtual environment, go to the directory where you want to create it and run the following command in the terminal:
    <pre><code>$ python3.10 -m venv &ltenvironment_name&gt</pre></code></li>
  <li> Once this has been done, the environment can be activated by running the following command:
    <pre><code>$ source &ltenvironment_name&gt/bin/activate</pre></code>
  If the environment has been activated correctly its name should appear in parentheses on the left of the user name in the terminal.</li>
  <li>Ensure pip is installed. If if not, follow instructions found <a href="https://pip.pypa.io/en/stable/installation/">here</a> to install it.</li>
  <li> To install the required packages, run the command:
    <pre><code>$ python3.10 -m pip install -r requirements.txt</pre></code></li>
</ul>
</section>
<section id="spacy-model">
<h4>Spacy model<a class="headerlink" href="#spacy-model" title="Permalink to this headline">¶</a></h4>
<p>Some of the tools used in the module require a language model to be donwloaded by the user. This can be done running the following command:</p>
  <pre><code>$ python3.10 -m spacy download en_core_web_lg</pre></code>
<p>The language model will be stored in the created virtual environment.</p>
</section>
</section>
</section>
<section id="running-the-notebooks">
<h2>Running the notebooks<a class="headerlink" href="#running-the-notebooks" title="Permalink to this headline">¶</a></h2>
<p>We can use jupyter notebook to run the jupyter notebooks that appear on the repository. To do so, we can run the following command:</p>
 <pre>
  <code>
    $ python3.10 -m ipykernel install --user --name  &ltenvironment_name&gt --display-name "&ltkernel_name&gt"
    $ python3.10 -m jupyter notebook &ltpath of the notebook we want to run&gt
  </code>
</pre>
<p>The first command will define a kernel, named &lt;kernel_name&gt;, which you must change to after opening jupyter notebook. The second command will open a jupyter notebook terminal on our explorer, where we can run the selected notebook.
We will give now instructions for running each one of the notebooks, depending on the datasets that we want to use in our models.</p>
<section id="classical-classifiers-ipynb">
<h3><a class="reference external" href="https://github.com/NEASQC/WP6_QNLP/blob/v0.3/neasqc_wp61/doc/tutorials/Classical_classifiers.ipynb">Classical_classifiers.ipynb</a><a class="headerlink" href="#classical-classifiers-ipynb" title="Permalink to this headline">¶</a></h3>
  <ul>
    <li>In cell[2], the argument of <code>loadData()</code> must be one of the following:
      <ul>
        <li><code>"../../data/dataset_vectorized_bert_uncased.json"</code></li>
        <li><code>"../../data/dataset_vectorized_bert_cased.json"</code></li>
      </ul>
    </li>
    <li>In cell[8], the argument of <code>loadData()</code> must be:
      <ul>
        <li><code>"../../data/dataset_vectorized_fasttext.json"</code></li>
      </ul>
    </li>
  </ul>
</section>
<section id="dataset-example-ipynb">
<h3><a class="reference external" href="https://github.com/NEASQC/WP6_QNLP/blob/v0.3/neasqc_wp61/doc/tutorials/Dataset_example.ipynb">Dataset_example.ipynb</a><a class="headerlink" href="#dataset-example-ipynb" title="Permalink to this headline">¶</a></h3>
  <ul>
    <li>In cell[3], the value of the variable <code>filename</code> must be one of the following:
      <ul>
        <li><code>"Complete_dataset.json"</code></li>
        <li><code>"dataset_vectorized_bert_cased.json"</code></li>
        <li><code>"dataset_vectorized_bert_uncased.json"</code></li>
        <li><code>"dataset_vectorized_fasttext.json"</code></li>
        <li><code>"Expanded_Transitive_dataset.json"</code></li>
        <li><code>"Reduced_words_complete_dataset.json"</code></li>
      </ul>
    </li>
  </ul>
</section>
<section id="single-sentence-example-ipynb">
<h3><a class="reference external" href="https://github.com/NEASQC/WP6_QNLP/blob/v0.3/neasqc_wp61/doc/tutorials/Single_sentence_example.ipynb">Single_sentence_example.ipynb</a><a class="headerlink" href="#single-sentence-example-ipynb" title="Permalink to this headline">¶</a></h3>
  <ul>
    <li>No dataset is input on this notebook. No restrictions when running the notebook.</li>
  </ul>
</section>
<section id="dressed-qnlp-demo-ipynb">
<h3><a class="reference external" href="https://github.com/NEASQC/WP6_QNLP/blob/v0.3/neasqc_wp61/models/quantum/alpha/Dressed_QNLP_demo.ipynb">Dressed_QNLP_demo.ipynb</a><a class="headerlink" href="#dressed-qnlp-demo-ipynb" title="Permalink to this headline">¶</a></h3>
  <ul>
    <li>In cell[24], the value of the variable <code>filename</code> can be any of the datasets:
      <ul>
        <li><code>"../../../data/Complete_dataset.json"</code></li>
        <li><code>'../../../data/dataset_vectorized_bert_cased.json"</code></li>
        <li><code>'../../../data/dataset_vectorized_bert_uncased.json"</code></li>
        <li><code>'../../../data/dataset_vectorized_fasttext.json"</code></li>
        <li><code>'../../../data/Expanded_Transitive_dataset.json"</code></li>
        <li><code>'../../../data/Reduced_words_complete_dataset.json"</code></li>
        <li><code>'../../../data/Reduced_words_transitive_dataset.json"</code></li>
      </ul>
    </li>
  </ul>
</section>
</section>
<section id="pre-alpha-functionalities">
<h2>Pre-alpha Functionalities<a class="headerlink" href="#pre-alpha-functionalities" title="Permalink to this headline">¶</a></h2>
<p>The main scope of the pre-alpha model is to build a variational quantum algorithm that makes sentence classification in categories True or False. The structure of the analyzed sentences will be:</p>
<ul>
  <li>NOUN-TRANSITIVE VERB-NOUN</li>
  <li>NOUN-INTRANSITIVE VERB</li>
  <li>NOUN-INTRANSITIVE VERB-PREPOSITION-NOUN</li>
</ul>
<section id="classical-classifiers">
<h3>Classical classifiers<a class="headerlink" href="#classical-classifiers" title="Permalink to this headline">¶</a></h3>
<p>Some classical classifiers are implemented in order to have a reference against which to compare our quantum solution.</p>
<ul>
  <li>K-nearest neighbors classifier from sklearn package applied to BERT embeddings.</li>
  <li>A feedforward neural network classifier from tensorflow package applied to BERT embeddings.</li>
  <li>A convolutional neural network classifier from tensorflow package applied to fasttext embeddings.</li>
</ul>
<p>After the models are trained we compute and compare their accuracies on the training and test dataset.</p>
</section>
<section id="variational-quantum-circuit-classfier">
<h3>Variational quantum circuit classfier<a class="headerlink" href="#variational-quantum-circuit-classfier" title="Permalink to this headline">¶</a></h3>
<p>A variational quantum circuit is built with initial random parameters. In the circuit, only one qubit is measured (post-selection), and the variational parameters will be optimized using the labeled sentences in dataset so that the state obtained when measuring the qubit coincides with the value of the sentence (FALSE=0, TRUE=1).</p>
<p>After the variational circuit is trained, the accuracy over the training and test dataset will be measured and compared.</p>
<p>As an additional feature, a function that guesses a missing word in a sentence is also included. The function is applied to a new dataset generated by removing a noun from each of the sentences in the initial dataset. For a given sentence, the function will be looking for the missing word between all the words that have been removed, and will select the one whose circuit gives the greater probability of obtaining the state corresponding to the meaning of the sentence.</p>
</section>
</section>
<section id="pre-alpha-documentation">
<h2>Pre-alpha Documentation<a class="headerlink" href="#pre-alpha-documentation" title="Permalink to this headline">¶</a></h2>
<p>Let’s briefly describe how the functionalities explained above are implemented.</p>
<p>The classical classifiers are implemented in the notebook <a class="reference external" href="https://github.com/NEASQC/WP6_QNLP/blob/v0.3/neasqc_wp61/doc/tutorials/Classical_classifiers.ipynb">Classical classifiers.ipynb</a>. The file <a class="reference external" href="https://github.com/NEASQC/WP6_QNLP/blob/v0.3/neasqc_wp61/models/classical/NNClassifier.py">NNClassifier.py</a> contains the class and functions used to prepare the data, build and train the convolutional and feedforward networks.</p>
<p>Regarding the variational quantum circuit, in the notebook <a class="reference external" href="https://github.com/NEASQC/WP6_QNLP/blob/v0.3/neasqc_wp61/doc/tutorials/Single_sentence_example.ipynb">Single_sentence_example.ipynb</a> we can find an example where the circuit parameters are optimized based on only one sentence. In the notebook <a class="reference external" href="https://github.com/NEASQC/WP6_QNLP/blob/v0.3/neasqc_wp61/doc/tutorials/Dataset_example.ipynb">Dataset_example.ipynb</a>, we can find an example on the variational algorithm trained with a whole dataset of sentences. We also can see there the implementation of a function to guess a missing word in a sentence.</p>
<p>The functions and classes used to implement the variational quantum circuit are taken from different files. In <a class="reference external" href="https://github.com/NEASQC/WP6_QNLP/blob/v0.3/neasqc_wp61/models/quantum/pre-alpha/dictionary.py">dictionary.py</a> we define classes that allow us to store the words than can appear on a sentence in dictionaries. Functions are defined that allow us to get and update the variational parameters associated to each word in the quantum circuit. Some of these functions are used in <a class="reference external" href="https://github.com/NEASQC/WP6_QNLP/blob/v0.3/neasqc_wp61/models/quantum/pre-alpha/sentence.py">sentence.py</a>, which provides the required tools to build the structure of the circuit that represents the sentence depending on its type and in some user-defined parameters. <a class="reference external" href="https://github.com/NEASQC/WP6_QNLP/blob/v0.3/neasqc_wp61/models/quantum/pre-alpha/circuit.py">ciruit.py</a> contains functions that build, simplify (by qubit contractions) and execute the variational circuit. A class to optimize the variational parameters of the circuit with respect to a sentence or dataset of sentences can be found on <a class="reference external" href="https://github.com/NEASQC/WP6_QNLP/blob/v0.3/neasqc_wp61/models/quantum/pre-alpha/optimizer.py">optimizer.py</a>. Finally, in <a class="reference external" href="https://github.com/NEASQC/WP6_QNLP/blob/v0.3/neasqc_wp61/models/quantum/pre-alpha/loader.py">loader.py</a> we can find functions that help in the processing of the datasets.</p>
</section>
<section id="alpha-functionalities">
<h2>Alpha Functionalities<a class="headerlink" href="#alpha-functionalities" title="Permalink to this headline">¶</a></h2>
<section id="sentence-dataset-and-model-purpose">
<h3>Sentence Dataset and Model Purpose<a class="headerlink" href="#sentence-dataset-and-model-purpose" title="Permalink to this headline">¶</a></h3>
<p>The datatset used is <a class="reference external" href="https://github.com/NEASQC/WP6_QNLP/blob/v0.3/neasqc_wp61/data/Complete_dataset.json">Complete_dataset.json</a>, which contains sentences of varying grammatical structure that are labelled as true or false. The idea is to train a model, using this dataset, that can classify unknown sentences as true or false.</p>
</section>
<section id="bert-sentence-embeddings">
<h3>BERT Sentence Embeddings<a class="headerlink" href="#bert-sentence-embeddings" title="Permalink to this headline">¶</a></h3>
<p>Using the BERT model, the word embeddings of each word in each sentence in an inputted dataset are calculated. These word embeddings serve as the initial parameters, after dimensionality reduction, of the parametrised quantum circuits that represent each sentence.</p>
</section>
<section id="discocat-circuits">
<h3>DisCoCat Circuits<a class="headerlink" href="#discocat-circuits" title="Permalink to this headline">¶</a></h3>
<p>Using the lambeq package, a tket parametrised quantum circuit is generated for each sentence in the dataset.</p>
</section>
<section id="dressed-quantum-circuit-for-sentence-classification">
<h3>Dressed Quantum Circuit for Sentence Classification<a class="headerlink" href="#dressed-quantum-circuit-for-sentence-classification" title="Permalink to this headline">¶</a></h3>
<p>The dressed quantum circuit forms our trainable PyTorch neural network. Initially the sentence embedding dimensionality is reduced to be compatible with the dimension of their respective parameters within the parametrised quantum circuits. This is achieved using sequential, cascasding linear transformations. Next the quantum circuit is run, after which the measurement outcomes are fed into a post-processing neural network which ultimately classifies the sentence as true or false.</p>
</section>
<section id="torch-optimisation">
<h3>Torch OPtimisation<a class="headerlink" href="#torch-optimisation" title="Permalink to this headline">¶</a></h3>
<p>The dressed quantum circuit model is trained using Torch.</p>
</section>
</section>
<section id="alpha-documentation">
<h2>Alpha Documentation<a class="headerlink" href="#alpha-documentation" title="Permalink to this headline">¶</a></h2>
<p>The alpha is currently implemented in one single jupyter notebook, <a class="reference external" href="https://github.com/NEASQC/WP6_QNLP/blob/v0.3/neasqc_wp61/models/quantum/alpha/Dressed_QNLP_demo.ipynb">Dressed_QNLP_demo.ipynb</a>. The beginning of the notebook contains a demonstration of how BERT sentence embeddings and parametrised quantum circuits can be generated using a small number of sample sentences. After this demonstration, and below the “Dressed Quantum Circuits” heading, two classes are defined which can be used in a pipeline that takes in our sentence dataset and generates a trainable model. After this generation, the model is trained using PyTorch.</p>
<p>WARNING: You must run the initial demonstration as many of the functions and imports are used in the definition of the actual model, later in the notebook.</p>
<section id="alpha-discussion">
<h3>Alpha Discussion<a class="headerlink" href="#alpha-discussion" title="Permalink to this headline">¶</a></h3>
<p>The idea behind this Jupyter Notebook is to extend the work found in https://pennylane.ai/qml/demos/tutorial_quantum_transfer_learning.html. In transfer learning, the first layers of a pretrained neural network are used to solve a different problem to that used to train the network, adding new layers to the model that specialise in that specific task.</p>
<p>We are using a BERT model to retrieve the context dependant embeddings for the words present in a sentence. Which layer is the best to retrieve the embeddings from is unclear, and it will need to be investigated. Once we have those vectors, they propagate through a feedforward network that first will reduce the dimensionality to an intermediate representation (in the notebook it is set to 20), and then the following layers will continue reducing the dimensionality of the vectors until reaching the number of parameters needed by the tensor representation of the quantum circuit offered by the Lambeq library for that word in a specific sentences.</p>
<p>Some benefits and issues of this approach are:</p>
<section id="benefits">
<h4>Benefits<a class="headerlink" href="#benefits" title="Permalink to this headline">¶</a></h4>
<ul>
  <li>Any sentence structure and word contained in the BERT model used can be processed by the full pipeline. No need to store the values of parameters for a dictionary</li>
  <li>It is possible to generalize to different NLP tasks</li>
  <li>If the dimensionaility of the category space is changed, the NN can be re-scaled to reuse the model for new circuit dimensionaility.</li>
</ul>  
</section>
<section id="issues">
<h4>Issues<a class="headerlink" href="#issues" title="Permalink to this headline">¶</a></h4>
<ul>
  <li>Pytket loses the tensor nature of parameters, giving an output consisting of a list of floats or simply counts -> Differentiable circuits in Pennylane could be a solution.</li>
  <li>It is not clear if we gain any quantum advantage with this methods, as a classical NN has to be trained.</li>
</ul>
</section>
</section>
</section>
<section id="classical-nlp">
<h2>Classical NLP<a class="headerlink" href="#classical-nlp" title="Permalink to this headline">¶</a></h2>
<p>A module implementing classical processing of the dataset.</p>
<p>The NNClassifier.py for the classical NLP module is located in the
<a class="reference external" href="https://github.com/NEASQC/WP6_QNLP/tree/v0.3/neasqc_wp61/models/classical">classical</a> subdirectory.</p>
<section id="classical-nlp-notebooks">
<h3>Classical NLP Notebooks<a class="headerlink" href="#classical-nlp-notebooks" title="Permalink to this headline">¶</a></h3>
<p>The notebooks for the classical NLP module are located in the
<a class="reference external" href="https://github.com/NEASQC/WP6_QNLP/tree/v0.3/neasqc_wp61/doc/tutorials">classical notebooks</a> subdirectory.</p>
</section>
</section>
<section id="benchmarking">
<h2>Benchmarking<a class="headerlink" href="#benchmarking" title="Permalink to this headline">¶</a></h2>
<section id="vectorizer">
<h3>Vectorizer<a class="headerlink" href="#vectorizer" title="Permalink to this headline">¶</a></h3>
<p>Services are found <a class="reference external" href="https://github.com/NEASQC/WP6_QNLP/tree/v0.3/neasqc_wp61/benchmarking/data_processing">here</a> for vectorizing using pretrained word embeddings.</p>
<p>The aim is to have vectorizing service detached from the rest of the library so that different vectorizing methods can easily be tested using the same interface.</p>
<p>Currently, vectorizing with <code class="docutils literal notranslate"><span class="pre">BERT</span></code> and <code class="docutils literal notranslate"><span class="pre">fastText</span></code> models are implemented.</p>
</section>
<section id="dataset-generation-example">
<h3>Dataset Generation Example<a class="headerlink" href="#dataset-generation-example" title="Permalink to this headline">¶</a></h3>
<section id="generating-the-animal-dataset">
<h4>Generating the animal dataset<a class="headerlink" href="#generating-the-animal-dataset" title="Permalink to this headline">¶</a></h4>
<p>Manual dataset generation isn’t necessary for running the Jupyter notebooks.
However, if needed for some different purpose, the dataset can be generated
using the following commands.</p>
<p>Run</p>
<pre>
  <code>
# Choose a seed number, e.g, 1337, to deterministically randomize the sentence order
./neasqc_wp61/benchmarking/data_processing/gen_animal_dataset.py --seed 1337 > outfile
  </code>
</pre>
<p>to generate a tab-separated file containing lines of the form
<code class="docutils literal notranslate"><span class="pre">&lt;sentence&gt;\t&lt;sentence_type&gt;\t&lt;truth_value&gt;</span></code> where <code class="docutils literal notranslate"><span class="pre">&lt;truth_value&gt;</span></code> is 1 if the sentence states a
fact that is true and 0 otherwise, and <code class="docutils literal notranslate"><span class="pre">&lt;sentence_type&gt;</span></code> denotes the sentence type, e.g., <code class="docutils literal notranslate"><span class="pre">NOUN-TVERB-NOUN</span></code>.</p>
</section>
</section>
</section>
<section id="data">
<h2>Data<a class="headerlink" href="#data" title="Permalink to this headline">¶</a></h2>
<p>This subdirectory contains the relevant datasets used in the models. For example, consider the <a class="reference external" href="https://github.com/NEASQC/WP6_QNLP/blob/v0.3/neasqc_wp61/data/Complete_dataset.json">complete_dataset.json</a>. In this dataset, a list of true or false labelled sentences of varying grammatical structure is contained. We can use this dataset in sentence classification training as in the alpha model.</p>
</section>
<section id="doc">
<h2>Doc<a class="headerlink" href="#doc" title="Permalink to this headline">¶</a></h2>
<p>In this subdirectory,relevant documentation and tutorials are contained. Tutorials come in the form of jupyter notebooks and cover the classical and pre-alpha models.</p>
</section>
<section id="models">
<h2>Models<a class="headerlink" href="#models" title="Permalink to this headline">¶</a></h2>
<p>Here we can find the Alpha and Pre-Alpha models.</p>
</section>
<section id="acknowledgements">
<h2>Acknowledgements<a class="headerlink" href="#acknowledgements" title="Permalink to this headline">¶</a></h2>
<p>This work is supported by the <a class="reference external" href="https://www.neasqc.eu">NEASQC</a> project, funded by the European Union’s Horizon 2020 programme, Grant Agreement No. 951821.</p>
<section id="license">
<h3>License<a class="headerlink" href="#license" title="Permalink to this headline">¶</a></h3>
<p>The <a class="reference external" href="https://github.com/NEASQC/WP6_QNLP/blob/v0.3/LICENSE">LICENSE</a> file contains the default license statement as specified in the proposal and partner agreement.</p>
</section>
</section>
</section>


          </div>
          
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
<h1 class="logo"><a href="index.html">WP6_QNLP</a></h1>








<h3>Navigation</h3>
<ul class="current">
<li class="toctree-l1 current"><a class="current reference internal" href="#">Quantum Natural Language Processing : NEASQC WP6.1</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#installing-locally">Installing locally</a></li>
<li class="toctree-l2"><a class="reference internal" href="#running-the-notebooks">Running the notebooks</a></li>
<li class="toctree-l2"><a class="reference internal" href="#pre-alpha-functionalities">Pre-alpha Functionalities</a></li>
<li class="toctree-l2"><a class="reference internal" href="#pre-alpha-documentation">Pre-alpha Documentation</a></li>
<li class="toctree-l2"><a class="reference internal" href="#alpha-functionalities">Alpha Functionalities</a></li>
<li class="toctree-l2"><a class="reference internal" href="#alpha-documentation">Alpha Documentation</a></li>
<li class="toctree-l2"><a class="reference internal" href="#classical-nlp">Classical NLP</a></li>
<li class="toctree-l2"><a class="reference internal" href="#benchmarking">Benchmarking</a></li>
<li class="toctree-l2"><a class="reference internal" href="#data">Data</a></li>
<li class="toctree-l2"><a class="reference internal" href="#doc">Doc</a></li>
<li class="toctree-l2"><a class="reference internal" href="#models">Models</a></li>
<li class="toctree-l2"><a class="reference internal" href="#acknowledgements">Acknowledgements</a></li>
</ul>
</li>
</ul>

<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="index.html">Documentation overview</a><ul>
      <li>Previous: <a href="index.html" title="previous chapter">Welcome to WP6_QNLP’s documentation!</a></li>
  </ul></li>
</ul>
</div>
<div id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false"/>
      <input type="submit" value="Go" />
    </form>
    </div>
</div>
<script>$('#searchbox').show(0);</script>








        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &copy;2022, Richard Wolf, Conor Dunne, Pablo Suarez Vieites.
      
      |
      Powered by <a href="http://sphinx-doc.org/">Sphinx 4.2.0</a>
      &amp; <a href="https://github.com/bitprophet/alabaster">Alabaster 0.7.12</a>
      
      |
      <a href="_sources/README.md.txt"
          rel="nofollow">Page source</a>
    </div>

    

    
  </body>
</html>